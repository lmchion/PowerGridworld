[H[2J[3Jrunning locally true
iteration set 1
env_set ['003']
env 003
training_iteration 1000 1000 1
CompletedProcess(args=['python', '-u', 'train_hs.py', '--stop-iters', '1000', '--stop-reward', '-0.5', '--num-cpus', '0', '--num-gpus', '1', '--local-dir', '/home/ec2-user/PowerGridworld/data/outputs/ray_results', '--max-episode-steps', '288', '--input-dir', '/home/ec2-user/PowerGridworld/data/inputs', '--last-checkpoint', 'None', '--training-iteration', '1000', '--scenario-id', '003'], returncode=0, stdout=b'on_experiment_end dumping the last result for validation..\nCheckpoint(local_path=/home/ec2-user/PowerGridworld/data/outputs/ray_results/PPO/PPO_003_fd01b_00000_0_framework=torch_2023-03-31_00-28-41/checkpoint_000004)\n', stderr=b"[INFO] train_hs.py:37: ARGS: {'env_name': 'buildings', 'system_load_rescale_factor': 0.6, 'max_episode_steps': 288, 'local_dir': '/home/ec2-user/PowerGridworld/data/outputs/ray_results', 'stop_timesteps': 10000000000, 'stop_iters': 1000, 'stop_reward': -0.5, 'run': 'PPO', 'framework': 'torch', 'num_gpus': 1, 'num_cpus': 0, 'num_samples': 1, 'log_level': 'DEBUG', 'node_ip_address': '127.0.0.1', 'input_dir': '/home/ec2-user/PowerGridworld/data/inputs', 'last_checkpoint': 'None', 'training_iteration': '1000', 'scenario_id': '003'}\n[INFO] train_hs.py:62: ENV CONFIG\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:45,708\tWARNING algorithm_config.py:596 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:45,728\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m /opt/conda/envs/gridworld_hs/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m   if not isinstance(terminated, (bool, np.bool8)):\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:45,905\tDEBUG rollout_worker.py:1948 -- Creating policy for default_policy\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:45,905\tDEBUG preprocessors.py:272 -- Creating sub-preprocessor for Box(-1.0, 1.0, (7,), float64)\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:45,905\tDEBUG preprocessors.py:272 -- Creating sub-preprocessor for Box(-1.0, 1.0, (2,), float64)\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:45,905\tDEBUG preprocessors.py:272 -- Creating sub-preprocessor for Box(-1.0, 1.0, (1,), float64)\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:45,906\tDEBUG preprocessors.py:272 -- Creating sub-preprocessor for Box(-1.0, 1.0, (2,), float64)\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:45,906\tDEBUG catalog.py:781 -- Created preprocessor <ray.rllib.models.preprocessors.DictFlatteningPreprocessor object at 0x7f449a599540>: Dict('ev-charging': Box(-1.0, 1.0, (7,), float64), 'other-devices': Box(-1.0, 1.0, (2,), float64), 'pv': Box(-1.0, 1.0, (1,), float64), 'storage': Box(-1.0, 1.0, (2,), float64)) -> (12,)\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:45,915\tINFO policy.py:1214 -- Policy (worker=local) running on 1 GPUs.\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:45,916\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:48,548\tDEBUG preprocessors.py:272 -- Creating sub-preprocessor for Box(-1.0, 1.0, (7,), float64)\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:48,549\tDEBUG preprocessors.py:272 -- Creating sub-preprocessor for Box(-1.0, 1.0, (2,), float64)\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:48,549\tDEBUG preprocessors.py:272 -- Creating sub-preprocessor for Box(-1.0, 1.0, (1,), float64)\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:48,549\tDEBUG preprocessors.py:272 -- Creating sub-preprocessor for Box(-1.0, 1.0, (2,), float64)\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:48,793\tDEBUG preprocessors.py:272 -- Creating sub-preprocessor for Box(-1.0, 1.0, (7,), float64)\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:48,794\tDEBUG preprocessors.py:272 -- Creating sub-preprocessor for Box(-1.0, 1.0, (2,), float64)\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:48,794\tDEBUG preprocessors.py:272 -- Creating sub-preprocessor for Box(-1.0, 1.0, (1,), float64)\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:48,794\tDEBUG preprocessors.py:272 -- Creating sub-preprocessor for Box(-1.0, 1.0, (2,), float64)\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:48,794\tINFO util.py:122 -- Using connectors:\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:48,795\tINFO util.py:123 --     AgentConnectorPipeline\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m         ObsPreprocessorConnector\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m         MeanStdObservationFilterAgentConnector\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m         StateBufferConnector\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m         ViewRequirementAgentConnector\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:48,795\tINFO util.py:124 --     ActionConnectorPipeline\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m         ConvertToNumpyConnector\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m         NormalizeActionsConnector\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m         ImmutableActionsConnector\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:48,795\tINFO rollout_worker.py:2043 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:48,795\tINFO rollout_worker.py:2044 -- Built preprocessor map: {'default_policy': None}\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:48,795\tINFO rollout_worker.py:760 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {'default_policy': <ray.rllib.utils.filter.MeanStdFilter object at 0x7f4491ab4d60>})\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:48,795\tDEBUG rollout_worker.py:855 -- Created rollout worker with env <ray.rllib.env.vector_env.VectorEnvWrapper object at 0x7f449a599540> (<TimeLimit<OrderEnforcing<PassiveEnvChecker<HSMultiComponentEnv<house-v0>>>>>), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:48,800\tINFO rollout_worker.py:908 -- Generating sample batch of size 288\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:50,170\tINFO rollout_worker.py:949 -- Completed sample batch:\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m \n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m { 'count': 288,\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((288, 8), dtype=float32, min=-0.011, max=0.013, mean=-0.0),\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m                                           'action_logp': np.ndarray((288,), dtype=float32, min=-12.736, max=-3.702, mean=-5.599),\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m                                           'actions': np.ndarray((288, 4), dtype=float32, min=-3.482, max=2.801, mean=0.021),\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m                                           'advantages': np.ndarray((288,), dtype=float32, min=-502.075, max=-0.006, mean=-168.606),\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m                                           'agent_index': np.ndarray((288,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m                                           'eps_id': np.ndarray((288,), dtype=int64, min=5.907247775983852e+17, max=5.907247775983852e+17, mean=5.907247775983852e+17),\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m                                           'infos': np.ndarray((288,), dtype=object, head={'pv': {'real_power': 0.0, 'timestamp': '06-29-2022 06:00:00', 'grid_cost': 0.25847, 'es_cost': 0, 'grid_power': 48, 'pv_power': 0.0, 'es_power': 0.0, 'pv_cost': 0.0, 'step_meta': [], 'state_of_charge': 0.0, 'time': 0.0, 'num_active_vehicles': 0.0, 'real_power_consumed': 0.0, 'real_power_demand': 0.0, 'mean_charge_rate_deficit': 0, 'real_power_unserved': 0.0, 'current_cost': 0.0, 'hvac_power': 0.30000000000000004, 'other_power': 0.30000000000000004}, 'storage': {'timestamp': '06-29-2022 06:00:00', 'grid_cost': 0.25847, 'es_cost': 0, 'grid_power': 48, 'pv_power': 0.0, 'es_power': 0.0, 'pv_cost': 0.0, 'step_meta': [], 'real_power': 0.0, 'state_of_charge': 0.0, 'time': 0.0, 'num_active_vehicles': 0.0, 'real_power_consumed': 0.0, 'real_power_demand': 0.0, 'mean_charge_rate_deficit': 0, 'real_power_unserved': 0.0, 'current_cost': 0.0, 'hvac_power': 0.30000000000000004, 'other_power': 0.30000000000000004}, 'ev-charging': {'time': 0.0, 'num_active_vehicles': 0.0, 'real_power_consumed': 0.0, 'real_power_demand': 0.0, 'mean_charge_rate_deficit': 0, 'real_power_unserved': 0.0, 'current_cost': 0.0, 'timestamp': '06-29-2022 06:00:00', 'grid_cost': 0.25847, 'es_cost': 0, 'grid_power': 48, 'pv_power': 0.0, 'es_power': 0.0, 'pv_cost': 0.0, 'step_meta': [], 'real_power': 0.0, 'state_of_charge': 0.0, 'hvac_power': 0.30000000000000004, 'other_power': 0.30000000000000004}, 'other-devices': {'hvac_power': 0.30000000000000004, 'other_power': 0.30000000000000004, 'timestamp': '06-29-2022 06:00:00', 'grid_cost': 0.25847, 'es_cost': 0, 'grid_power': 48, 'pv_power': 0.0, 'es_power': 0.0, 'pv_cost': 0.0, 'step_meta': [], 'real_power': 0.0, 'state_of_charge': 0.0, 'time': 0.0, 'num_active_vehicles': 0.0, 'real_power_consumed': 0.0, 'real_power_demand': 0.0, 'mean_charge_rate_deficit': 0, 'real_power_unserved': 0.0, 'current_cost': 0.0}}),\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m                                           'new_obs': np.ndarray((288, 12), dtype=float32, min=-2.4, max=12.53, mean=0.461),\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m                                           'obs': np.ndarray((288, 12), dtype=float32, min=-2.4, max=12.53, mean=0.461),\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m                                           'rewards': np.ndarray((288,), dtype=float32, min=-15.449, max=0.0, mean=-3.319),\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m                                           't': np.ndarray((288,), dtype=int64, min=0.0, max=287.0, mean=143.5),\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m                                           'terminateds': np.ndarray((288,), dtype=bool, min=0.0, max=1.0, mean=0.003),\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m                                           'truncateds': np.ndarray((288,), dtype=bool, min=0.0, max=1.0, mean=0.003),\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m                                           'unroll_id': np.ndarray((288,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m                                           'value_targets': np.ndarray((288,), dtype=float32, min=-502.071, max=0.0, mean=-168.604),\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m                                           'vf_preds': np.ndarray((288,), dtype=float32, min=-0.008, max=0.01, mean=0.002)}},\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m   'type': 'MultiAgentBatch'}\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m \n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:50,171\tDEBUG train_ops.py:156 -- == sgd epochs for default_policy ==\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:50,386\tDEBUG filter_manager.py:34 -- Synchronizing filters ...\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:50,387\tDEBUG filter_manager.py:55 -- Updating remote filters ...\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:50,387\tDEBUG algorithm.py:2300 -- synchronized filters: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {'default_policy': <ray.rllib.utils.filter.MeanStdFilter object at 0x7f4491ab4d60>})\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:51,930\tDEBUG train_ops.py:156 -- == sgd epochs for default_policy ==\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:52,144\tDEBUG filter_manager.py:34 -- Synchronizing filters ...\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:52,145\tDEBUG filter_manager.py:55 -- Updating remote filters ...\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:52,145\tDEBUG algorithm.py:2300 -- synchronized filters: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {'default_policy': <ray.rllib.utils.filter.MeanStdFilter object at 0x7f4491ab4d60>})\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:53,836\tDEBUG train_ops.py:156 -- == sgd epochs for default_policy ==\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:54,051\tDEBUG filter_manager.py:34 -- Synchronizing filters ...\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:54,052\tDEBUG filter_manager.py:55 -- Updating remote filters ...\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:54,052\tDEBUG algorithm.py:2300 -- synchronized filters: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {'default_policy': <ray.rllib.utils.filter.MeanStdFilter object at 0x7f4491ab4d60>})\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:55,880\tDEBUG train_ops.py:156 -- == sgd epochs for default_policy ==\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:56,095\tDEBUG filter_manager.py:34 -- Synchronizing filters ...\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:56,096\tDEBUG filter_manager.py:55 -- Updating remote filters ...\n\x1b[2m\x1b[36m(PPO pid=30171)\x1b[0m 2023-03-31 00:28:56,096\tDEBUG algorithm.py:2300 -- synchronized filters: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {'default_policy': <ray.rllib.utils.filter.MeanStdFilter object at 0x7f4491ab4d60>})\n")
Finished 003 process in 27.548096895217896 seconds.
last_checkpoint /home/ec2-user/PowerGridworld/data/outputs/ray_results/PPO/PPO_003_fd01b_00000_0_framework=torch_2023-03-31_00-28-41/checkpoint_000004
